{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Punctuation Prediction Demo Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R85wZvYEXF9M"
      },
      "source": [
        "**Imports and pip installation for the demo**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ymj_UZCPPit",
        "outputId": "8dabe3f6-5984-4c0b-d947-eefda6fb5a29"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB-c5jrgYk4_"
      },
      "source": [
        "**Loading The Bert Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMogEsixYnYh"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Andrianos/bert-base-greek-punctuation-prediction-finetuned\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Andrianos/bert-base-greek-punctuation-prediction-finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcPgzdw0XjUB"
      },
      "source": [
        "**Samples to Try**\n",
        "Note: This is an uncased Transformer, capitals are ignored\n",
        "\n",
        "Writing your own sample to try:\n",
        "Add all in one string, use words seperated by spaces, do not add punctuation. \n",
        "\n",
        "The model will then add the punctuation and you can see the final result in the last slot.\n",
        "\n",
        "Feel free to add any further procedure to post process text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY_bp3goaedk"
      },
      "source": [
        "# two_sentences = 'προσεκτικά στον δρόμο θα σε περιμένω'\n",
        "# question = 'τι θα φας για βραδινό'\n",
        "# multiple_statements = 'κύριε μαυροκέφαλε εσπασέν η αντέννα του ίντερνετ θα πάρω τηλέφωνο την cyta'\n",
        "news =  'αμεση ήταν η επέμβαση ναυαγοσωστών του ΛΕΜ15 στην παραλία απέναντι από το παλιό εθνικό γήπεδο το απόγευμα του Σαββάτου στο κάλεσμα ανηλίκου λουόμενου για βοήθεια ο νεαρός ενώ κολυμπούσε με φίλο του από τους κυματοθραύστες προς την ακτή λίγο πριν τις 6 λόγω κόπωσης κινδύνευσε με πνιγμό και κάλεσε σε βοήθεια ναυαγοσώστης του ΛΕΜ15 έσπευσε άμεσα και μετέφερε τον νεαρό με ασφάλειά στην ακτή συγχαρητήρια εύγε στον λεβέντη'\n",
        "orig_tokens = news.split()\n",
        "sentence = \" \".join(orig_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN9fFzZNZ6u0"
      },
      "source": [
        "**Predicting Punctuation using the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2h0734cPdVu"
      },
      "source": [
        "tokenized_sentence = tokenizer.encode(sentence)\n",
        "input_ids = torch.tensor([tokenized_sentence])\n",
        "with torch.no_grad():\n",
        "    output = model(input_ids)\n",
        "predictions = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "punctuation_points = ['.', ',', ';', '-', ':', '0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFTY-XitRb8z"
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "new_tokens, new_labels = [], []\n",
        "for token, label_idx in zip(tokens, label_indices[0]):\n",
        "    if token.startswith(\"##\"):\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "    else:\n",
        "        new_labels.append(punctuation_points[label_idx])\n",
        "        new_tokens.append(token)\n",
        "\n",
        "new_tokens = new_tokens[1:-1]\n",
        "new_labels = new_labels[1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llD63BUAawJJ"
      },
      "source": [
        "**Outputting into a single text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10GDsudWcAFE"
      },
      "source": [
        "def to_single_string(tokens, predictions):\n",
        "    final_string = \"\"\n",
        "    for i in range(len(tokens)):\n",
        "        final_string = final_string + tokens[i]\n",
        "        if predictions[i] != '0':\n",
        "            final_string = final_string + predictions[i]\n",
        "        if predictions[i] != '-':\n",
        "            final_string = final_string + \" \"\n",
        "    return final_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0JQE6An1ed9",
        "outputId": "ab05af35-5443-4540-eec1-9fefb2f4b443"
      },
      "source": [
        "print(to_single_string(new_tokens, new_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "αμεση ηταν η επεμβαση ναυαγοσωστων του λεμ15 στην παραλια απεναντι απο το παλιο εθνικο γηπεδο το απογευμα του σαββατου. στο καλεσμα ανηλικου λουομενου για βοηθεια ο νεαρος, ενω κολυμπουσε με φιλο του απο τους κυματοθραυστες προς την ακτη λιγο πριν τις 6, λογω κοπωσης, κινδυνευσε με πνιγμο και καλεσε σε βοηθεια. ναυαγοσωστης του λεμ15 εσπευσε αμεσα και μετεφερε τον νεαρο με ασφαλεια στην ακτη. συγχαρητηρια, ευγε στον λεβεντη \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}